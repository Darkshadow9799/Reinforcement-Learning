{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q learning Algo P1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdN6Aw_w-jbo",
        "outputId": "1e2cf39a-605d-4228-dec2-9714aac490a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhvOFxv6-sq-"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUg8X8Zo_E-s"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e047iciJ_NjO",
        "outputId": "2269055b-2387-4bac-90ec-bc332a1d8ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acion_space_size=env.action_space.n\n",
        "state_space_size=env.observation_space.n\n",
        "\n",
        "q_table=np.zeros((state_space_size,acion_space_size))\n",
        "print(q_table)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDEi7twi_-SZ"
      },
      "source": [
        "num_episodes=10000\n",
        "max_steps_per_episode=100\n",
        "\n",
        "learning_rate=0.1\n",
        "discount_rate=0.99\n",
        "\n",
        "exploration_rate=1\n",
        "max_exploration_rate=1\n",
        "min_exploration_rate=0.01\n",
        "exploration_decay_rate=0.001"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uq6SvzwY813"
      },
      "source": [
        "# Q Learning Algorithm:\n",
        "* Step 1: Start from the starting state for each new episode in the loop.\n",
        "* Step 2: Set the exploration rate threhold randomly between 0 to 1.\n",
        "* Step 3: If the threshold value > exploration rate then eploit the data, else explore.\n",
        "* Step 4: Update the Q table.\n",
        "* Step 5: Update Exploration Rate.\n",
        "* Step 6: Update the state to new state.\n",
        "* Step 7: Cal the reward for the current state.\n",
        "* Step 8: Repeat step 1 to 7 for N number of episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtCG02qaWMxK"
      },
      "source": [
        "# Exploration And Exploitation:\n",
        "It is used for determining whether to explore or to eploit the information gain from the exploration. 1st the exploration rate is set to 1 and epsilon i.e. eploration rate threshold which is selected randomly between 0 and 1 and if the epsilon > exploration rate then the exploitation is performed else eploration is done.\n",
        "\n",
        "# Q-Function:\n",
        "* Action Value Function:\n",
        "  \\begin{eqnarray*} q_{\\pi }\\left( s,a\\right) &=&E_{\\pi }\\left[ G_{t}\\mid S_{t}=s,A_{t}=a \\rule[-0.05in]{0in}{0.2in}\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right] \\text{.} \\end{eqnarray*}\n",
        "\n",
        "* State Value Function:\n",
        "\\begin{eqnarray*} v_{\\pi }\\left( s\\right) &=&E_{\\pi}\\left[\n",
        "                                                \\rule[-0.05in]{0in}{0.2in}G_{t}\\mid S_{t}=s\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s\\right] \\text{.} \\end{eqnarray*}\n",
        "\n",
        "* Here gamma value denotes the dicount rate.\n",
        "* After each episode the exploration decays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NumLmTzApds",
        "outputId": "23948325-6db4-4e17-8102-a30839b79c85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reward_all_episodes=[]\n",
        "\n",
        "# Q-learning Algo\n",
        "for episode in range(num_episodes):\n",
        "  state=env.reset()\n",
        "  \n",
        "  done=False\n",
        "  rewards_current_episode=0\n",
        "  for steps in range(max_steps_per_episode):\n",
        "\n",
        "    #Exploration-Exploitation Trade-Off\n",
        "    exploration_rate_threshold = random.uniform(0, 1)\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "        action = np.argmax(q_table[state,:]) \n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "      \n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Update Q-Table\n",
        "    q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "    learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "\n",
        "    state = new_state\n",
        "    rewards_current_episode+=reward\n",
        "\n",
        "    if done==True:\n",
        "      break\n",
        "    \n",
        "  # Exploration rate decay\n",
        "  exploration_rate = min_exploration_rate + \\\n",
        "    (max_exploration_rate - min_exploration_rate) * np.exp(-max_exploration_rate*episode)\n",
        "\n",
        "  reward_all_episodes.append(rewards_current_episode)\n",
        "\n",
        "# Cal and print the reward per 1000 rounds\n",
        "rewards_per_thousand_episodes = np.split(np.array(reward_all_episodes),num_episodes/1000)\n",
        "count=1000\n",
        "\n",
        "print(\"AVERAGE\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "  print(count,\":\",str(sum(r/1000)))\n",
        "  count+=1000\n",
        "\n",
        "print(\"Q-Table\")\n",
        "print(q_table)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVERAGE\n",
            "1000 : 0.6560000000000005\n",
            "2000 : 0.6630000000000005\n",
            "3000 : 0.6700000000000005\n",
            "4000 : 0.6570000000000005\n",
            "5000 : 0.6930000000000005\n",
            "6000 : 0.6840000000000005\n",
            "7000 : 0.6580000000000005\n",
            "8000 : 0.6790000000000005\n",
            "9000 : 0.6620000000000005\n",
            "10000 : 0.6590000000000005\n",
            "Q-Table\n",
            "[[0.57963188 0.4771334  0.49038846 0.48804845]\n",
            " [0.22495918 0.18200858 0.19301685 0.47773254]\n",
            " [0.3325047  0.23784946 0.23238586 0.23964138]\n",
            " [0.05627423 0.         0.         0.        ]\n",
            " [0.61141183 0.44746712 0.28698367 0.35600107]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.2204046  0.12594695 0.12890787 0.09725564]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.40533845 0.40030661 0.35843433 0.64263667]\n",
            " [0.44142429 0.66956398 0.27251087 0.37697208]\n",
            " [0.62610389 0.29431128 0.37065792 0.27536528]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.28802673 0.36430952 0.73798783 0.48583438]\n",
            " [0.72893219 0.89707131 0.75862475 0.72473532]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_kZ5UAMHOTz",
        "outputId": "d6edc8a8-f832-472e-8bf3-5020b81782bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for episode in range(3):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  print(\"Episode\", episode+1)\n",
        "  time.sleep(1)\n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    action=np.argmax(q_table[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "    if done:\n",
        "     clear_output(wait=True)\n",
        "     env.render()\n",
        "     if reward==1:\n",
        "       print(\"You reached the goal\")\n",
        "       time.sleep(3)      \n",
        "     else:\n",
        "      print(\"You fell in the hole\")\n",
        "      time.sleep(3)\n",
        "     clear_output(wait=True)\n",
        "     break\n",
        "     \n",
        "    state = new_state\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "You fell in the hole\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR7cP_mrWFt6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}