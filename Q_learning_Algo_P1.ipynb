{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q learning Algo P1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdN6Aw_w-jbo",
        "outputId": "3c27980c-ad07-420d-9bb1-b1b55d1b17bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhvOFxv6-sq-"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUg8X8Zo_E-s"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e047iciJ_NjO",
        "outputId": "055bc58f-03b0-4b24-87db-3ed2aa2048db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acion_space_size=env.action_space.n\n",
        "state_space_size=env.observation_space.n\n",
        "\n",
        "q_table=np.zeros((state_space_size,acion_space_size))\n",
        "print(q_table)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uq6SvzwY813"
      },
      "source": [
        "# Q Learning Algorithm:\n",
        "* Step 1: Start from the starting state for each new episode in the loop.\n",
        "* Step 2: Set the exploration rate threhold randomly between 0 to 1.\n",
        "* Step 3: If the threshold value > exploration rate then eploit the data, else explore.\n",
        "* Step 4: Update the Q table.\n",
        "* Step 5: Update Exploration Rate.\n",
        "* Step 6: Update the state to new state.\n",
        "* Step 7: Cal the reward for the current state.\n",
        "* Step 8: Repeat step 1 to 7 for N number of episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtCG02qaWMxK"
      },
      "source": [
        "# Exploration And Exploitation:\n",
        "It is used for determining whether to explore or to eploit the information gain from the exploration. 1st the exploration rate is set to 1 and epsilon i.e. eploration rate threshold which is selected randomly between 0 and 1 and if the epsilon > exploration rate then the exploitation is performed else eploration is done.\n",
        "\n",
        "# Q-Function:\n",
        "* Action Value Function:\n",
        "  \\begin{eqnarray*} q_{\\pi }\\left( s,a\\right) &=&E_{\\pi }\\left[ G_{t}\\mid S_{t}=s,A_{t}=a \\rule[-0.05in]{0in}{0.2in}\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right] \\text{.} \\end{eqnarray*}\n",
        "\n",
        "* State Value Function:\n",
        "\\begin{eqnarray*} v_{\\pi }\\left( s\\right) &=&E_{\\pi}\\left[\n",
        "                                                \\rule[-0.05in]{0in}{0.2in}G_{t}\\mid S_{t}=s\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s\\right] \\text{.} \\end{eqnarray*}\n",
        "\n",
        "* Here gamma value denotes the dicount rate.\n",
        "* After each episode the exploration decays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NumLmTzApds",
        "outputId": "6aaf4ee4-54e2-45dd-b655-3aee010c2fa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_episodes=10000\n",
        "max_steps_per_episode=100\n",
        "\n",
        "learning_rate=0.1\n",
        "discount_rate=0.99\n",
        "\n",
        "exploration_rate=1\n",
        "max_exploration_rate=1\n",
        "min_exploration_rate=0.01\n",
        "exploration_decay_rate=0.01\n",
        "\n",
        "reward_all_episodes=[]\n",
        "\n",
        "# Q-learning Algo\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    rewards_current_episode = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        exploration_rate_threshold = random.uniform(0, 1)\n",
        "        if exploration_rate_threshold > exploration_rate:\n",
        "            action = np.argmax(q_table[state,:])\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state,:]) - q_table[state, action])\n",
        "        \n",
        "\n",
        "        state = new_state\n",
        "        rewards_current_episode += reward\n",
        "        if done: break\n",
        "\n",
        "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
        "    reward_all_episodes.append(rewards_current_episode)\n",
        "\n",
        "# Cal and print the reward per 1000 rounds\n",
        "rewards_per_thousand_episodes = np.split(np.array(reward_all_episodes),num_episodes/1000)\n",
        "count=1000\n",
        "\n",
        "print(\"AVERAGE\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "  print(count,\":\",str(sum(r/1000)))\n",
        "  count+=1000\n",
        "\n",
        "print(\"Q-Table\")\n",
        "print(q_table)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVERAGE\n",
            "1000 : 0.5030000000000003\n",
            "2000 : 0.6810000000000005\n",
            "3000 : 0.6720000000000005\n",
            "4000 : 0.6550000000000005\n",
            "5000 : 0.6790000000000005\n",
            "6000 : 0.6940000000000005\n",
            "7000 : 0.7100000000000005\n",
            "8000 : 0.6520000000000005\n",
            "9000 : 0.6600000000000005\n",
            "10000 : 0.7000000000000005\n",
            "Q-Table\n",
            "[[0.49928852 0.47352678 0.47099726 0.47359682]\n",
            " [0.27616284 0.31310282 0.30970402 0.40467693]\n",
            " [0.35299893 0.27288119 0.27512663 0.28895406]\n",
            " [0.00760728 0.16167901 0.06886011 0.05937051]\n",
            " [0.5212925  0.32339043 0.38123739 0.41720369]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.31752935 0.10593243 0.14491849 0.10605798]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.35843278 0.40412746 0.4812062  0.56876155]\n",
            " [0.52819476 0.61984626 0.44777148 0.41601555]\n",
            " [0.60404711 0.44390203 0.32085985 0.32504734]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.38436464 0.50456909 0.72571972 0.58000908]\n",
            " [0.69060102 0.87984644 0.73295972 0.75644582]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_kZ5UAMHOTz",
        "outputId": "e41c3f7b-e985-4143-a14b-18875175967a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for episode in range(3):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  print(\"Episode\", episode+1)\n",
        "  time.sleep(1)\n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    action=np.argmax(q_table[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "    if done:\n",
        "     clear_output(wait=True)\n",
        "     env.render()\n",
        "     if reward==1:\n",
        "       print(\"You reached the goal\")\n",
        "       time.sleep(3)      \n",
        "     else:\n",
        "      print(\"You fell in the hole\")\n",
        "      time.sleep(3)\n",
        "     clear_output(wait=True)\n",
        "     break\n",
        "     \n",
        "    state = new_state\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "You reached the goal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQX6jY_K6II4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}